\section{Clustering}


\subsection{Clustering with Gaussian Mixture Model}
One way to cluster data is with a Gaussian Mixture Model (GMM). This model uses a
probabilistic estimation of densities trained, in this report, with the
Expectation Maximizaion (EM) algorithm.\\
The GMM uses multivariate normal distributions as the distribution, which means
each cluster has assigned a mean ($\mu$) and a covariance matrix ($\Sigma$) to it.
This means that a given observation, depending on these two parameters and the
observation itself, has a probability of belonging to each cluster. Then, the
cluster witch the observation is most likely to belong to is the cluster the
observation is assigned to.\\
The first thing to do is find out the amount of clusters the GMM should aim for.
This is done with cross-validation (where the negative log-likehood should be
as low as possibe), Akaike's information Criterion (AIC) and Bayesian Information
Criterion (BIC). AIC and BIC should be low as well, so the following plot, showing
cross-validation, AIC and BIC for different amount of clusters, is helpful when
determining number of clusters.\\
\includegraphics[width=\textwidth]{Figure_1.png}
The BIC and negative log-likelihood are both low at k=2, while AIC continues
decreasing up to k=10. The two criterions penalize model complexity in different ways,
and it is often the case that AIC prefers the more complex model. With this in mind,
k=2 is used for the modelling in this section.\\
The 2 clusters projected down on the 2 attributes explaining the higest amount
of variance (insulin and glucose, responsible for more than 95\% of the total
variance) can be seen below:\\
\includegraphics[width=\textwidth]{Figure_2.png}
Qualitatively inspecting the data shows cluster 1 (red circles), which one
will most likely belong when having both low glucose and insulin (centered
at the bottom left red star). Also, cluster 1 seems, in this plane, to be more
an ellipsoid than a circle. Cluster 2 is more spread out and more circular, with
center at higher glucose and insulin levels.

\subsection{Hierarchical clustering}
Hierarchical clustering is a deterministic way of clustering observations. The
point is start off by assigning each observation to its own cluster and then
merge clusters that are closest to each other. Before initializing the algorith,
it is neccessary to  




\section{Outlierssssssssss}

\section{Association Miningsssssssss}



\appendix
\section{Distribution of responsibilities}
\subsection{Regression}
Linear regression by Andreas,
ANN by Marcus,
comparison by Marcus.
\subsection{Classification}
Explanations by Hildibjørg,
decision tree and KNN by Andreas,
RNN by Marcus,
comparison text by Hildibjørg,
comparison statistics by Andreas.
\subsection{Previous work}
By Hildibjørg.
