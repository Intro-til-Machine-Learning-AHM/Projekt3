\section{Clustering}


\subsection{Clustering with Gaussian Mixture Model}
One way to cluster data is with a Gaussian Mixture Model (GMM). This model uses a
probabilistic estimation of densities trained, in this report, with the
Expectation Maximizaion (EM) algorithm.\\
The GMM uses multivariate normal distributions as the distribution, which means
each cluster has assigned a mean ($\mu$) and a covariance matrix ($\Sigma$) to it.
This means that a given observation, depending on these two parameters and the
observation itself, has a probability of belonging to each cluster. Then, the
cluster witch the observation is most likely to belong to is the cluster the
observation is assigned to.\\
The first thing to do is find out the amount of clusters the GMM should aim for.
This is done with cross-validation (where the negative log-likehood should be
as low as possibe), Akaike's information Criterion (AIC) and Bayesian Information
Criterion (BIC). AIC and BIC should be low as well, so the following plot, showing
cross-validation, AIC and BIC for different amount of clusters, is helpful when
determining number of clusters.\\
\includegraphics[width=\textwidth]{Figure_1.png}
The BIC and negative log-likelihood are both low at k=2, while AIC continues
decreasing up to k=10. The two criterions penalize model complexity in different ways,
and it is often the case that AIC prefers the more complex model. With this in mind,
k=2 is used for the modelling in this section.\\
The 2 clusters projected down on the 2 attributes explaining the higest amount
of variance (insulin and glucose, responsible for more than 95\% of the total
variance) can be seen below:\\
\includegraphics[width=\textwidth]{Figure_2.png}
Qualitatively inspecting the data shows cluster 1 (red circles), which one
will most likely belong when having both low glucose and insulin (centered
at the bottom left red star). Also, cluster 1 seems, in this plane, to be more
an ellipsoid than a circle. Cluster 2 is more spread out and more circular, with
center at higher glucose and insulin levels.
Because only 2 attributes are presented in the plot, not all the variance is
explained, which is why the clustering seems a bit inconsistent in 2 dimensions.

\subsection{Hierarchical clustering}
Hierarchical clustering is a deterministic way of clustering observations. The
point is start off by assigning each observation to its own cluster and then
merge clusters that are closest to each other. Before initializing the algorithm,
it is neccessary to specify how the distance between clusters is defined. In this
report, the distance is defined by Ward's linkage function (page 236 in the
book). The hierarchical clustering of the data, this time standardized can be seen
below represented by a dendrogram with 6 levels:\\
\includegraphics[width=\textwidth]{Figure_3.png}
Outliers can usually be found by visual inspection of a dendrogram, where the
outliers will join other clusters late. This is, however, difficult to see in
this dendrogram, as the bottom is not visible. In general, the dendrogram seems
quite symmetrical which is a good sign, and the longest vertical lines suggests
2 clusters best describes the data. Plotted against the glucose and insulin attributes,
the following scatter-plot emerges:\\
\includegraphics[width=\textwidth]{Figure_4.png}
Qualitatively, some of the same structures can be seen as in the scatter plot
of the GMM. Again, some of the observations assigned to, say, cluster 2
surrounded by cluster 1's must somehow be different in a direction not show
on the scatter-plot.

\subsection{Quality of clustering}
In this section, the clusters' validity will be evaluated based on Rand statistic,
Jaccard coefficient and Normalized Mutual Information (NMI). The hierarchical clustering
will be cut off making 2 clusters (which, based on where the lonest vertical lines
are in the dendrogram, seems reasonable) and will be compared to the GMM with
Ward linkage. The table below gives an overview of the performances:\\
\begin{table}[h]
\centering
\begin{tabular}{ccccc}
    & Rand & Jaccard & NMI\\
GMM & 0.59 & 0.44 & 0.15 \\
HC & 0.58 & 0.45 & 0.09 \\
\end{tabular}
%\caption{Clustering evaluation scores}
\end{table}

The Rand and Jaccard scores are almost the same, however, the NMI score is better
for the GMM.

\section{Outlierssssssssss}

\section{Association Miningsssssssss}

\begin{center}
 \begin{tabular}{||c c||}
 \hline
 Attribute number & Attribute \\ [0.5ex]
 \hline\hline
 0 and 9 & Number of times pregnant \\
 \hline
 1 and 10 & Plasma glucose concentration, GTT \\
 \hline
 2 and 11 & Diastolic blood pressure (mm Hg) \\
 \hline
 3 and 12 & Triceps skin fold thickness (mm) \\
 \hline
 4 and 13 & 2-Hour serum insulin ($\mu$ U / ml) \\
 \hline
 5 and 14 & Body mass index Weight in (kg / (Height in $m^2$)) \\
 \hline
 6 and 15 & Diabetes pedigree function \\
 \hline
 7 and 16 & Age (Years) \\
 \hline
 8 and 17 & Class variable, whether or not the person had diabetes (0 or 1) \\ [1ex]
 \hline
\end{tabular}
\end{center}



\appendix
\section{Distribution of responsibilities}
\subsection{Clustering}

\subsection{Outlier detection/Anomaly detection}

\subsection{Association mining}

\subsection{Previous work}
