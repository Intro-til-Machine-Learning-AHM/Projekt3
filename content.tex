\section{Clustering}


\subsection{Clustering with Gaussian Mixture Model}
One way to cluster data is with a Gaussian Mixture Model (GMM). This model uses a
probabilistic estimation of densities trained, in this report, with the
Expectation Maximizaion (EM) algorithm.\\
The GMM uses multivariate normal distributions as the distribution, which means
each cluster has assigned a mean ($\mu$) and a covariance matrix ($\Sigma$) to it.
This means that a given observation, depending on these two parameters and the
observation itself, has a probability of belonging to each cluster. Then, the
cluster witch the observation is most likely to belong to is the cluster the
observation is assigned to.\\
The first thing to do is find out the amount of clusters the GMM should aim for.
This is done with cross-validation (where the negative log-likehood should be
as low as possibe), Akaike's information Criterion (AIC) and Bayesian Information
Criterion (BIC). AIC and BIC should be low as well, so the following plot, showing
cross-validation, AIC and BIC for different amount of clusters, is helpful when
determining number of clusters.\\
\includegraphics[width=\textwidth]{Figure_1.png}
The BIC and negative log-likelihood are both low at k=2, while AIC continues
decreasing up to k=10. The two criterions penalize model complexity in different ways,
and it is often the case that AIC prefers the more complex model. With this in mind,
k=2 is used for the modelling in this section.\\
The 2 clusters projected down on the 2 attributes explaining the higest amount
of variance (insulin and glucose, responsible for more than 95\% of the total
variance) can be seen below:\\
\includegraphics[width=\textwidth]{Figure_2.png}
Qualitatively inspecting the data shows cluster 1 (red circles), which one
will most likely belong when having both low glucose and insulin (centered
at the bottom left red star). Also, cluster 1 seems, in this plane, to be more
an ellipsoid than a circle. Cluster 2 is more spread out and more circular, with
center at higher glucose and insulin levels.
Because only 2 attributes are presented in the plot, not all the variance is
explained, which is why the clustering seems a bit inconsistent in 2 dimensions.

\subsection{Hierarchical clustering}
Hierarchical clustering is a deterministic way of clustering observations. The
point is start off by assigning each observation to its own cluster and then
merge clusters that are closest to each other. Before initializing the algorithm,
it is neccessary to specify how the distance between clusters is defined. In this
report, the distance is defined by Ward's linkage function (page 236 in the
book). The hierarchical clustering of the data, this time standardized can be seen
below represented by a dendrogram with 6 levels:\\
\includegraphics[width=\textwidth]{Figure_3.png}
Outliers can usually be found by visual inspection of a dendrogram, where the
outliers will join other clusters late. This is, however, difficult to see in
this dendrogram, as the bottom is not visible. In general, the dendrogram seems
quite symmetrical which is a good sign, and the longest vertical lines suggests
2 clusters best describes the data. Plotted against the glucose and insulin attributes,
the following scatter-plot emerges:\\
\includegraphics[width=\textwidth]{Figure_4.png}
Qualitatively, some of the same structures can be seen as in the scatter plot
of the GMM. Again, some of the observations assigned to, say, cluster 2
surrounded by cluster 1's must somehow be different in a direction not show
on the scatter-plot.

\subsection{Quality of clustering}
In this section, the clusters' validity will be evaluated based on Rand statistic,
Jaccard coefficient and Normalized Mutual Information (NMI). The hierarchical clustering
will be cut off making 2 clusters (which, based on where the lonest vertical lines
are in the dendrogram, seems reasonable) and will be compared to the GMM with
Ward linkage. The table below gives an overview of the performances:\\
\begin{table}[h]
\centering
\begin{tabular}{ccccc}
    & Rand & Jaccard & NMI\\
GMM & 0.59 & 0.44 & 0.15 \\
HC & 0.58 & 0.45 & 0.09 \\
\end{tabular}
%\caption{Clustering evaluation scores}
\end{table}

The Rand and Jaccard scores are almost the same, however, the NMI score is better
for the GMM.

\section{Outlierssssssssss}

\section{Association Miningsssssssss}

\begin{center}
 \begin{tabular}{||c c||}
 \hline
 Attribute number & Attribute \\ [0.5ex]
 \hline\hline
 0 and 9 & Number of times pregnant \\
 \hline
 1 and 10 & Plasma glucose concentration, GTT \\
 \hline
 2 and 11 & Diastolic blood pressure (mm Hg) \\
 \hline
 3 and 12 & Triceps skin fold thickness (mm) \\
 \hline
 4 and 13 & 2-Hour serum insulin ($\mu$ U / ml) \\
 \hline
 5 and 14 & Body mass index Weight in (kg / (Height in $m^2$)) \\
 \hline
 6 and 15 & Diabetes pedigree function \\
 \hline
 7 and 16 & Age (Years) \\
 \hline
 8 and 17 & Class variable, whether or not the person had diabetes (0 or 1) \\ [1ex]
 \hline
\end{tabular}
\end{center}

The association mining gives us an idea of which parameters have an association with each other and if they follow a rule of association.
What is interesting are the parameters which have a high association with each other.
There are two categories that can describe this association:

\textbf{High support:} Which describes if a parameter is rather often involved in the dataset and what are it's probability of getting chosen.

\textbf{High confidence:} Which describes if a parameter is chosen, what other parameter is then most likely to also be chosen, and are there more than one parameter that is likely to get chosen and what are it's/their probability of getting chosen.

When simulating the association mining we set our minSup to 40 and minConf to 80.
It should be noted that not all of our results are listed below due to the support being only 50% or less which we didn't find interesting.

Our results are as follows:
\begin{center}
 \begin{tabular}{||c||}
 \hline
 Frequent itemset number: & Percentage \\ [0.5ex]
 \hline\hline
 17 & Sup. 66.8367 \\
 \hline
 9 & Sup. 54.3367 \\
 \hline
 10 and 17 & Sup. 43.3673 \\
 \hline
 16 and 17 & Sup. 43.3673 \\
 \hline
 13 and 17 & Sup. 42.0918 \\
 \hline
 9 and 17 & Sup. 41.3265 \\
 \hline
 16 and 9 & Sup. 41.3265 \\ [1ex]
 \hline
\end{tabular}
\end{center}

\begin{center}
 \begin{tabular}{||c||}
 \hline
 Association Rule number: & Percentage \\ [0.5ex]
 \hline\hline
 17 <- 10 & Conf. 86.2944, Sup. 43.3673 \\
 \hline
 16 <- 9 and 17 & Conf. 85.8025, Sup. 35.4592 \\
 \hline
 17 <- 16 and 9 & Conf. 85.8025, Sup. 35.4592 \\ [1ex]
 \hline
\end{tabular}
\end{center}

There can clearly be seen pattern here. Parameter 17, or the class variable of having diabetes, clearly has an association with the other parameters.
Which would the reason for it having so many associations with the other parameters make sense.
What then is interesting is the association between parameters 16 and 9. Which are the parameters age and number of times pregnant.
It would indicate that 



\appendix
\section{Distribution of responsibilities}
\subsection{Clustering}

\subsection{Outlier detection/Anomaly detection}

\subsection{Association mining}

\subsection{Previous work}
